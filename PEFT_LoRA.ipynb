{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ggZutaiRRwat"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Parameter-Efficient Fine-Tuning (PEFT) Methods\n",
        "\n",
        "Parameter Efficient Fine Tuning (PEFT) refers to a suite of techniques used to fine tune models in more efficient, \"scrappier\" ways. The core insight behind these methods is that models normally can have their behaviors ajdusted by adjusting a **much, much smaller** subset of parameters than tradtional fine tuning.\n",
        "\n",
        "In this notebook, we'll dive deep into the most important parameter-efficient fine-tuning techniques for large transformer-based models. We'll explore LoRA, DoRA, and Adapter methods, understanding both their theoretical foundations and practical implementations.\n",
        "\n",
        "------\n",
        "\n",
        "#### Why do we need PEFT?\n",
        "In a typical fine-tuning paradigm, we would update all parameters of a pre-trained model. For a 7B parameter model like Llama-2 for instance, this would mean storing gradients and optimizer states for *all 7 billion parameters* - requiring massive GPU memory and computational resources that most practitioners simply don't have access to.\n",
        "\n",
        "PEFT methods solve this by updating only a small subset of parameters (often <1% of the total!) - yet they still achieve performance comparable to full fine-tuning. These methods take advantage of the fact that most of a large model's knowledge and abilities **doesn't** need to be changed or adjusted to adapt to new tasks.\n",
        "\n",
        "PEFT is an umbrella term, and lots of different specific methods are different ways of doing parameter efficient fine tuning. We can't cover nearly all of them in this notebook, but we'll go in depth on the most used, including:\n",
        "\n",
        "- **Low-Rank Adaptation (LoRA)** - The foundation of modern PEFT that uses matrix decomposition\n",
        "- **Weight-Decomposed Low-Rank Adaptation (DoRA)** - A more sophisticated approach that separates magnitude and direction\n",
        "- **Adapter Methods** - The original PEFT approach using small bottleneck layers\n",
        "\n",
        "We'll also touch on some others, giving more breif descriptions.\n",
        "\n",
        "---\n",
        "\n",
        "To the best of my ability, this will include practical implementations with real code examples, and guidance on when to use each method and choosing the right approach for a given use case."
      ],
      "metadata": {
        "id": "7BU6xCGtC6FX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Basic Setup\n",
        "\n",
        "Let's start by preparing everything we'll need to run the code in this notebook."
      ],
      "metadata": {
        "id": "zfNWR9W-KSIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installs + Imports\n",
        "\n",
        "We'll set this up to run on Google Colab, which will need some additional installs. Below, we also import all packages needed for this code."
      ],
      "metadata": {
        "id": "JA525CdvKpF5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DQiR2nng-wI1"
      },
      "outputs": [],
      "source": [
        "# Core ML libraries\n",
        "!pip install -q transformers>=4.35.0\n",
        "!pip install -q accelerate>=0.24.0\n",
        "!pip install -q datasets>=2.14.0\n",
        "\n",
        "# PEFT libraries - we'll use the official PEFT library and also implement from scratch\n",
        "!pip install -q peft>=0.7.0\n",
        "!pip install -q bitsandbytes>=0.41.0  # For efficient quantization if needed\n",
        "\n",
        "# Visualization and analysis\n",
        "!pip install -q matplotlib seaborn\n",
        "!pip install -q plotly  # For interactive plots\n",
        "\n",
        "# Optional: for more advanced examples\n",
        "!pip install -q wandb  # For experiment tracking (optional)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's import everything we'll need\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    TaskType,\n",
        "    PeftModel\n",
        ")\n",
        "from datasets import Dataset\n",
        "import math\n",
        "from typing import Optional, Dict, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('default')"
      ],
      "metadata": {
        "id": "JLsGQUoALf4D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if we have CUDA available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ðŸ”§ Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mZO9NF4Lkjh",
        "outputId": "3e4385d5-7419-46fe-e439-3f4d25712206"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”§ Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. **Method 1 -** LoRA (Low-Rank Adaptation)\n",
        "\n",
        "The first method that we'll cover is LoRA, which relies on doing low-rank updates to the weight matrices that comprise a model."
      ],
      "metadata": {
        "id": "JWavMJeVOv1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The Background**\n",
        "LoRA is based on a simple but powerful insight: when we fine-tune a pre-trained model, the weight updates often have low \"intrinsic rank\".\n",
        "\n",
        "That is to say, how many independent directions of change are actually meaningful in any given update matrix $\\Delta W = W_{fine-tuned} - W_{original}$ is small. Even when $\\Delta W$ might be a huge matrix (say 4096Ã—4096), most of the meaningful changes during fine tuning are captured by much fewer dimensions.\n",
        "\n",
        "----\n",
        "\n",
        "So, where did this idea come from?\n",
        "\n"
      ],
      "metadata": {
        "id": "ggZutaiRRwat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. **Prior Theoretical Work on Intrinsic Dimensionality**\n",
        "Before LoRA, several papers established that neural networks often have much lower \"intrinsic dimensionality\" than their parameter count suggests. In particular, **Li et al. (2018)** and **Aghajanyan et al. (2020)** both invesigated training smaller subspaces of these models and found success adapting models therein, showing that the *effective* number of parameters needed for adaptation might be much smaller than the total parameter count.\n",
        "\n",
        "#### 2. **The Key Empirical Discovery**\n",
        "The LoRA authors **(Hu et al., 2021)** took this insight and attempted to measure the intrinsic rank of the weight updates during fine-tuning.\n",
        "\n",
        "To do this, they:\n",
        "- Fine-tuned GPT-3 on various tasks using full fine-tuning\n",
        "- Computed the weight update matrices $\\Delta W = W_{fine-tuned} - W_{original}$\n",
        "- Performed Singular Value Decomposition (SVD) on these update matrices and analyzed how the values were distributed\n",
        "\n",
        "#### 3. **The Findings**\n",
        "What they found was that the weight update matrices had **very low effective ranks.** In fact,\n",
        "\n",
        "- Most singular values were tiny (close to zero)\n",
        "- Only a small number of singular values (often < 100) contained most of the \"signal\"\n",
        "- And it held across different model sizes, tasks, and layers\n",
        "\n",
        "This result wasn't mathematically guaranteed nor obvious. Fine-tuning could theoretically require complex, high-dimensional change. And while over-parameterization suggested *some* redundancy, it was not known that that fine-tuning updates would be *universally* low-rank in a task, model, and optimization-independent way."
      ],
      "metadata": {
        "id": "Ja3j_mK6cuwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **The Mathematical Intuition**\n",
        "\n",
        "Of course, there are several fundamental reasons why this low-rank structure might emerge and make good mathematical sense.\n",
        "\n",
        "##### **1. _The Over-Parameterization Hypothesis_**\n",
        "It's been hypothesized that large language models are massively over-parameterized for any *single* task. This over-parameterization means that: many parameters are redundant for the specific adaptation task, and changes to different parameters become highly correlated.\n",
        "\n",
        "##### **2. _Feature Reuse and Composition_**\n",
        "Pre-trained models already contain rich, hierarchical feature representations. So fine-tuning typically can just rely on:\n",
        "\n",
        "- **Reweighting existing features** rather than learning new ones from scratch\n",
        "- **Combining existing patterns** in new ways rather than creating entirely new patterns\n",
        "- **Adjusting decision boundaries** rather than learning new feature detectors\n",
        "\n",
        "This also means adaptations can be expressed as linear combinations of *existing* feature directions.\n",
        "\n",
        "##### **3. _Task Similarity_**\n",
        "Most fine-tuning tasks share underlying structure with the pre-training objective. For instance, any new language task will involve:\n",
        "\n",
        "- language understanding and generation\n",
        "- use of common syntactic and semantic patterns\n",
        "\n",
        "The main difference is often in *style* or *domain* rather than fundamental capabilities, so the adaptation primarily involves adjusting the \"mixing weights\" of existing capabilities rather than learning entirely new ones.\n",
        "\n",
        "##### **4. _Gradient Flow and Optimization Dynamics_**\n",
        "During fine-tuning, gradients tend to flow along the directions that were already \"activated\" during pre-training, meaning that:\n",
        "\n",
        "1. Parameters that were important for pre-training are more likely to be updated\n",
        "2. Updates tend to be correlated across layers (if one layer needs to change, related layers need complementary changes)\n",
        "\n",
        "This type of optimization will naturally find low-dimensional paths through the parameter space."
      ],
      "metadata": {
        "id": "MlI7UxQpa6-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The Core Mathematical Idea**\n",
        "\n",
        "Now that we understand *why* updates might need to only be low-rank, ;et's talk about *how* we do low-rank updated in practice.\n",
        "\n",
        "Instead of updating the full weight matrix $W \\in \\mathbb{R}^{d \\times k}$ during fine-tuning, LoRA represents the update as:\n",
        "\n",
        "$$h = W_0 x + \\Delta W x = W_0 x + BA x$$\n",
        "\n",
        "Here:\n",
        "- $W_0$ is the original **frozen** pre-trained weight matrix\n",
        "- $B \\in \\mathbb{R}^{d \\times r}$ and $A \\in \\mathbb{R}^{r \\times k}$ are trainable **low-rank matrices**\n",
        "- $r \\ll \\min(d,k)$ is the rank of these matrices (typically 1-256)\n",
        "- $\\Delta W = BA$ represents the low-rank update\n",
        "\n",
        "> Remember, if B is a {4096 x 4} matrix, and A is a {4 x 4096} matrix (rank 4), the *size of the update* to W is still {4096 x 4096} !\n",
        "\n",
        "---\n",
        "\n",
        "With this update, the number of trainable parameters drops from $d \\times k$ to $(d + k) \\times r$\n",
        "\n",
        "For a typical transformer layer with $d=k=4096$ and $r=64$, that means:\n",
        "- **Full fine-tuning**: $4096 \\times 4096 = 16.8M$ parameters\n",
        "- **LoRA**: $(4096 + 4096) \\times 64 = 524K$ parameters  \n",
        "\n",
        "- -> 97% fewer parameters!\n",
        "\n",
        "\n",
        "### Initialization Strategy (Critical!)\n",
        "\n",
        "LoRA uses a specific initialization strategy that's crucial for training stability:\n",
        "- Matrix $A$ is initialized with small random Gaussian values: $A \\sim \\mathcal{N}(0, \\sigma^2)$\n",
        "- Matrix $B$ is initialized to **zero**: $B = 0$\n",
        "- This ensures $\\Delta W = BA = 0$ at initialization\n",
        "\n",
        "This means training starts with the exact original pre-trained model behavior, then gradually learns the adaptation. A scaling factor $\\alpha/r$ is often applied to control the magnitude of updates:\n",
        "\n",
        "$$h = W_0 x + \\frac{\\alpha}{r} BA x$$\n",
        "\n",
        "Where $\\alpha$ is a hyperparameter (often 16 or 32) that controls how much the LoRA adaptation affects the original model.\n",
        "\n",
        "### The Mathematical Intuition\n",
        "\n",
        "In linear algebra terms, if most of the \"action\" in $\\Delta W$ happens along just $r$ directions (where $r$ is small), then we can write:\n",
        "\n",
        "$$\\Delta W = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T$$\n",
        "\n",
        "This is exactly the low-rank decomposition! The $u_i$ and $v_i$ are the key directions of change, and $\\sigma_i$ are their strengths. LoRA approximates this by learning $B$ and $A$ such that $\\Delta W \\approx BA$, where $B$ captures the output directions and $A$ captures the input directions of these changes."
      ],
      "metadata": {
        "id": "Met22bJQStdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from typing import Optional\n",
        "\n",
        "class LoRALayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A Low-Rank Adaptation layer that can wrap any linear layer.\n",
        "\n",
        "    This implementation shows the core LoRA concept:\n",
        "    - Keep original weights frozen\n",
        "    - Add low-rank adaptation via two smaller matrices\n",
        "    - Apply scaling to control adaptation strength\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 original_layer: nn.Linear,\n",
        "                 rank: int = 4,\n",
        "                 alpha: float = 32.0,\n",
        "                 dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "        self.scaling = alpha / rank  # This is the Î±/r scaling factor\n",
        "\n",
        "        # Get dimensions from the original layer\n",
        "        in_features = original_layer.in_features\n",
        "        out_features = original_layer.out_features\n",
        "\n",
        "        # Freeze the original layer\n",
        "        self.original_layer = original_layer\n",
        "        for param in self.original_layer.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Create the low-rank matrices A and B\n",
        "        # A: (rank, in_features) - initialized with small random values\n",
        "        # B: (out_features, rank) - initialized to zero\n",
        "        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.01)\n",
        "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
        "\n",
        "        # Optional dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Original computation: Wâ‚€x\n",
        "        original_output = self.original_layer(x)\n",
        "\n",
        "        # LoRA computation: (Î±/r) * B * A * x\n",
        "        # We compute this as: (Î±/r) * B * (A * x) for efficiency\n",
        "        lora_output = self.dropout(x) @ self.lora_A.T  # (batch, rank)\n",
        "        lora_output = lora_output @ self.lora_B.T      # (batch, out_features)\n",
        "        lora_output = lora_output * self.scaling       # Apply Î±/r scaling\n",
        "\n",
        "        # Combine: Wâ‚€x + (Î±/r)BAx\n",
        "        return original_output + lora_output\n",
        "\n",
        "    def get_delta_weights(self):\n",
        "        \"\"\"\n",
        "        Returns the actual Î”W = (Î±/r)BA matrix for analysis\n",
        "        \"\"\"\n",
        "        return self.scaling * (self.lora_B @ self.lora_A)\n",
        "\n",
        "# Let's test this with a simple example\n",
        "print(\"ðŸ”§ Creating a test linear layer and its LoRA version...\")\n",
        "\n",
        "# Original linear layer\n",
        "original = nn.Linear(512, 256)\n",
        "print(f\"Original layer parameters: {sum(p.numel() for p in original.parameters()):,}\")\n",
        "\n",
        "# LoRA version\n",
        "lora_layer = LoRALayer(original, rank=16, alpha=32)\n",
        "trainable_params = sum(p.numel() for p in lora_layer.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in lora_layer.parameters())\n",
        "\n",
        "print(f\"LoRA trainable parameters: {trainable_params:,}\")\n",
        "print(f\"LoRA total parameters: {total_params:,}\")\n",
        "print(f\"Parameter reduction: {(1 - trainable_params/total_params)*100:.1f}%\")\n",
        "\n",
        "# Test forward pass\n",
        "x = torch.randn(32, 512)  # batch_size=32, input_dim=512\n",
        "output = lora_layer(x)\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "# Show that Î”W starts at zero (due to B initialization)\n",
        "delta_w = lora_layer.get_delta_weights()\n",
        "print(f\"Initial Î”W norm: {delta_w.norm().item():.6f} (should be ~0)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEugFYwIlXZF",
        "outputId": "384babf2-d410-4ac6-f3c8-66f38c58e0f4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”§ Creating a test linear layer and its LoRA version...\n",
            "Original layer parameters: 131,328\n",
            "LoRA trainable parameters: 12,288\n",
            "LoRA total parameters: 143,616\n",
            "Parameter reduction: 91.4%\n",
            "Input shape: torch.Size([32, 512])\n",
            "Output shape: torch.Size([32, 256])\n",
            "Initial Î”W norm: 0.000000 (should be ~0)\n"
          ]
        }
      ]
    }
  ]
}
